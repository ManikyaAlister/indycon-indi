---
title: "Exploratory analyses: demographics"
author: "Anonymised for peer review" #Eee Von Soh and Manikya 
date: "2024-01-16"
fig_height: 4 
output:
  bookdown::pdf_document2: 
    keep_tex: true
    df_print: kable
    latex_engine: xelatex
toc: true
number-sections: true
---

```{r setup, include = FALSE}
# Load Libraries
library(here)
library(tidyverse)
library(dplyr)
library(ggpubr)

# make default that code is not shown in knit, no warnings, and no mesages
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# endsure latex is rendered 
#knitr::opts_knit$kable.force.latex

# load data
load(here("data/clean/all_data_clean.Rdata"))

# All data
data <- all_data[[1]] 
# Only data for dependent and independent condition
d_independence <- data %>% 
  filter(consensus != "contested")

# Only data for independent and contested condition
d_consensus_only <- data %>% 
  filter(consensus != "dependent")

```



```{r, include = FALSE}
demographics <- all_data$demographics
```

```{r}
# Since Age is a Character, convert it to a numeric
demographics$demographics_age <- as.integer(demographics$demographics_age)

# Mean age
age_stats <- demographics |> 
  summarize(mean_age = mean(demographics_age))
```

```{r include = FALSE}
ggplot(demographics, aes(x = demographics_age)) +  
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  geom_vline(aes(xintercept = mean_age), age_stats, color = "red", linewidth = 1) +   # Geom_vline -> visualise the mean
  labs(title = "Age Distribution", x = "Age", y = "Frequency") +
  theme_minimal()
```

```{r include = FALSE}
# Frequency table for gender
gender_counts <- table(demographics$demographics_gender) 

# Convert the table to a data frame
gender_counts_df <- as.data.frame(gender_counts)

# Rename the variables in the data frame
colnames(gender_counts_df) <- c("Gender", "Count")

# Calculate percentage
gender_counts_df$Percentage <- gender_counts_df$Count / sum(gender_counts_df$Count) * 100

# Sort the data frame by percentage in descending order
gender_counts_df <- gender_counts_df[order(-gender_counts_df$Percentage), ]

# Keep only the top 2 rows
gender_counts_top2 <- head(gender_counts_df, 2)

# Create a pie chart with labels for the top 2 highest percentages
ggplot(gender_counts_df, aes(x = "", y = Count, fill = Gender)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y") +
  geom_text(data = gender_counts_top2,
            aes(label = paste0(round(Percentage, 1), "%")),
            position = position_stack(vjust = 0.5),
            color = "white",
            size = 4) +
  labs(title = "Gender Distribution", fill = "Gender") +
  theme_minimal() +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        panel.grid = element_blank(),
        plot.title = element_text(hjust = 0.5))
```


```{r include = FALSE}
table_education <- table(demographics$demographics_education)

# Convert the table to a data frame
df_education <- as.data.frame(table_education)

# Rename the variables in the data frame
colnames(df_education) <- c("Education", "Count")

# Create a bar plot
ggplot(df_education, aes(x = Education, y = Count, fill = Education)) +
  geom_bar(stat = "identity", color = "black", alpha = 0.6) +
  labs(title = "Education Distribution", x = "Education Level", y = "Frequency") +
  theme_minimal()
```


```{r}
merge_data_dem <- merge(data, demographics, by = c("participant"))  %>%
    mutate(politcal_group = factor(political_group, levels = c("Left wing","Centre", "Right wing")))



```

\pagebreak
# Overview

In this supplementary material we explore whether any of the individual differences in consensus strength may be able to be explained by demographic characteristics. We initially plot each of the relationships of interest, then follow up with a quantitative model comparison using Bayesian linear mixed effects models with interactions between consensus and each respective demographic variable. None of these analyses were preregistered. These analyses are meant for offering <b>speculation</b> only, with the goal of motivating future research to follow up these possible explanations. There are many aspects of the sample and design that make these analyses potentially unreliable, which we detail later.  

Lines in the scatter plots below indicate a linear model fit to the points of the respective color using the geom_smooth function in R with method lm. The shaded area indicates the 95% Confidence Interval. Note that these linear models are not the same models that are fit in the model comparison later. In the bar plots, error bars are 95% confidence intervals. 

```{r}
# Define a function for plotting a basic grouped bar plot with optional ordering
plot_grouped_bar <-
  function(by_variable,
           column,
           x_label,
           usage_ordering = NULL,
           label_angle = 0,
           show_n = FALSE) {
    colour_scale <- c("purple", "darkblue", "red")
    if (!is.null(usage_ordering)) {
      plot <-
        ggplot(data = by_variable, aes(
          x = factor({
            {
              column
            }
          }, levels = usage_ordering),
          y = mean_update,
          fill = consensus
        )) +
        geom_bar(position = "dodge",
                 stat = "identity",
                 color = "black",
                 alpha = .8) +
        geom_errorbar(
          aes(ymin = mean_update - se, ymax = mean_update + se),
          position = position_dodge(0.9),
          width = 0.2
        ) +
        labs(x = x_label, y = "Mean Update (Post - Prior)") +
        theme_minimal() +
        theme(axis.text.x = element_text(angle = label_angle, hjust = 1))
    } else {
      plot <-
        ggplot(data = by_variable, aes(x = {
          {
            column
          }
        }, y = mean_update, fill = consensus)) +
        geom_bar(position = "dodge",
                 stat = "identity",
                 color = "black",
                 alpha = .8) +
        geom_errorbar(
          aes(ymin = mean_update - se, ymax = mean_update + se),
          position = position_dodge(0.9),
          width = 0.2
        ) +
        labs(x = x_label, y = "Mean Update (Post - Prior)") +
        theme_minimal() +
        theme(axis.text.x = element_text(angle = label_angle, hjust = 1))
    }
    
    plot <- plot +
      scale_color_manual(values = colour_scale) +
      scale_fill_manual(values = colour_scale)
    return(plot)
  }
```

```{r}
# Add labels to plot
add_num_obs <- function(data, aggregate_data, plot, column, col_name, usage_ordering = NULL, vjust = -1){
  
  count_num_participants <- data %>%
    group_by({{column}}) %>%
    summarise(n_participants = n()/60)
  
  # Merge the count data back into the by_social_media data frame
  by_data_w_count <- merge(aggregate_data, count_num_participants, by = col_name, all.x = TRUE)
  
  if (!is.null(usage_ordering)){
    new_plot <- plot + geom_text(data = by_data_w_count, 
            aes(x = factor({{column}}, levels = usage_ordering), y = 0, label = n_participants), position = position_dodge(1), vjust = vjust,colour = "darkgrey", inherit.aes = FALSE)
  }else{
    new_plot <- plot + geom_text(data = by_data_w_count, 
            aes(x = {{column}}, y = 0, label = n_participants), position = position_dodge(1), vjust = vjust, inherit.aes = FALSE, colour = "darkgrey")
  }

  return(new_plot)
}
```

```{r}
## Add Jittered Points to Plot
add_jitter <- function(data, plot, dodge_width = 1, width = 0.2, alpha = .1){
  
  new_plot <- plot + geom_jitter(data = data, aes(y = update, colour = consensus), position = position_jitterdodge(dodge.width = dodge_width), alpha = alpha) 
  
  return(new_plot)
}
```
\pagebreak
# Education
```{r}
ed_order <- c("secondary","ged","vocational","bachelors","masters","doctorate")

by_education <- merge_data_dem %>%
  group_by(consensus, demographics_education) %>%
summarise(mean_update = mean(update),n_obs  = n()/60, sd = sd(update), se = sd / sqrt(n()))

plot_education <- plot_grouped_bar(by_education, demographics_education, x_label = "Education", usage_ordering = ed_order)
plot_education_with_num_obs <- add_num_obs(merge_data_dem, by_education, plot_education, demographics_education, "demographics_education", vjust = -6)
```
Participants could select a number of options to indicate their level of education. In the current sample, "secondary"  "masters"  "bachelors"  "ged"   "vocational", and "doctorate" were selected. Since there are a large number of discrete categories, it makes sense to try and create subgroups so that it is easier to interpret, and so that there is more power when modeling. One logical sub group is whether or not the participant went to university or not. Although this is not perfect, it could be informative since half of the sources used in the consensus were said to be from a university, so people who have a university education might interpret information from these sources differently. 

```{r}
by_uni_ed <-  merge_data_dem %>%
  group_by(consensus, university_education) %>%
summarise(mean_update = mean(update),n_obs  = n()/60, sd = sd(update), se = sd / sqrt(n()))

plot_uni_education <- plot_grouped_bar(by_uni_ed, university_education, x_label = "University Educated?")
plot_unieducation_with_num_obs <- add_num_obs(merge_data_dem, by_uni_ed, plot_uni_education, university_education, "university_education", vjust = -6)


```

```{r education, fig.cap="Consensus by education. Numbers indicate the number of participants in each group. Error bars in this plot (and in all bar plots presented here) represent standard errors."}
ggarrange(plot_education_with_num_obs, plot_unieducation_with_num_obs, common.legend = TRUE, ncol = TRUE)
```

Looking at Figure \@ref(fig:education), there is some variability among each education category (top panel), though this does not appear to be obviously related to the number of years of education, and the sample sizes are quite inconsistent. The bottom panel shows that the consensus effects appeared to be equivalent regardless of university education. 



\pagebreak

# Social Media
```{r}
# Constant
USAGE_ORDERING <- c("daily", "weekly", "monthly", "rarely", "never")
```

```{r, message = FALSE, include = FALSE}
# Ignore this
by_facebook <- merge_data_dem %>%
  group_by(consensus, demographics_facebook) %>%
summarise(mean_update = mean(update),n_obs  = n()/60, sd = sd(update), se = sd / sqrt(n()))
```

```{r, message = FALSE, include = FALSE}
# Ignore this
count_facebook_usage <- merge_data_dem %>%
  group_by(demographics_facebook) %>%
  summarise(n_participants = n()/60)
```

```{r, message = FALSE, include = FALSE}
# Ignore this

# Merge the count data back into the by_facebook data frame
by_facebook_merge <- merge(by_facebook, count_facebook_usage, by = "demographics_facebook", all.x = TRUE)
```

```{r, include = FALSE}
# Ignore this
plotA <- ggplot(data = by_facebook, aes(x = factor(demographics_facebook, levels = USAGE_ORDERING), y = mean_update, fill = consensus)) +
  geom_bar(position = "dodge", stat = "identity", color = "black") + 
  labs(x = "Facebook Usage", y = "Mean Update (Post - Prior)") + 
  theme_minimal()
```

```{r, include = FALSE}
# Ignore this
plotA + geom_text(data = by_facebook_merge, 
            aes(x = factor(demographics_facebook, levels = USAGE_ORDERING), y = 0, label = n_participants),
            position = position_dodge(1), vjust = -22, inherit.aes = FALSE)
```

```{r, message = FALSE}
by_facebook <- merge_data_dem %>%
  group_by(consensus, demographics_facebook) %>%
summarise(mean_update = mean(update),n_obs  = n()/60, sd = sd(update), se = sd / sqrt(n()))

plot_facebook <- plot_grouped_bar(by_facebook, demographics_facebook, x_label = "Facebook Usage", usage_ordering = USAGE_ORDERING)

```
```{r}
plot_facebook_with_num_obs <- add_num_obs(merge_data_dem, by_facebook, plot_facebook, demographics_facebook, col_name = "demographics_facebook", usage_ordering = USAGE_ORDERING, vjust = -5)

```

```{r, message = FALSE}
by_twitter <- merge_data_dem %>%
  group_by(consensus, demographics_twitter) %>%
summarise(mean_update = mean(update),n_obs  = n()/60, sd = sd(update), se = sd / sqrt(n()))

plot_twitter <- plot_grouped_bar(by_twitter, demographics_twitter, x_label = "Twitter (X) Usage", usage_ordering = USAGE_ORDERING)

```

```{r}
plot_twitter_with_num_obs <- add_num_obs(merge_data_dem, by_twitter, plot_twitter, demographics_twitter, "demographics_twitter", usage_ordering =  USAGE_ORDERING, vjust = -5)
```

```{r facebook-twitter, fig.cap= "Consensus by frequency of Facebook and Twitter/X usage." }
ggarrange(
plot_twitter_with_num_obs,
plot_facebook_with_num_obs,
common.legend = TRUE,
legend = "bottom",
ncol = 1)
```

Figure \@ref(fig:facebook-twitter)  shows belief updating in each consensus condition as a function of how often participants reported using Facebook and Twitter/X respectively. One observation is that participants who used social media less appeared to have a stronger preference for independence compared to dependence. One problem that arises for any further analysis is that while these are technically categorical data, they, in theory, map onto a numeric value of how often participants use each platform each year. If we can transform these categories into numeric estimates of their social media use we can 1) aggregate both Facebook and Twitter/X into a single social media variable and 2) assess whether there is a linear relationship between how often people use social media and the extent to which they are persuaded by different kinds of consensus. 

The approach that we took was to create a transformed "Proportionate Social Media" variable, where we translated each category into a rough estimate of how much they use each social media platform every year. This is extremely noisy, but may provide some insight on the relationship between social media use and consensus effects. Specifically, we translate the scale such that daily = 365, weekly = 52, monthly = 12, rarely = 1, and never = 0. We then aggregated the two social medias such that, for example, if a participant selected daily for both, their proportionate social media score would be 730. 


```{r}
checkRelationship = function(data = merge_data_dem, rm_conds = c("dependent", "contested"), variable, var_label){
  plot_list <- NULL
  
  for ( i in 1:length(rm_conds)){
    rm_cond <- rm_conds[i]
      if (rm_cond == "contested"){
    colour_scale <- c("blue", "red")
  } else if (rm_cond == "dependent"){
    colour_scale <- c("purple", "red")
  }
  
  plot_list[[i]] <- data %>%
    filter(consensus != rm_cond) %>%
    group_by(participant, consensus) %>%
    summarise(update = mean(update), var = mean(get(variable))) %>%
    ggplot(aes(x = var, y = update, colour = consensus)) +
    labs(x = var_label, y = "Belief Update (Post - Pre)") +
    geom_point() +
    geom_smooth(method = "lm", aes(fill = consensus))+
    scale_colour_manual(values = colour_scale)+
    scale_fill_manual(values = colour_scale)+
    theme_bw()+
    theme(axis.title = element_text(size = 10))
  }
  
combined_plot <- ggarrange(plotlist = plot_list, legend = "bottom")  
combined_plot

}

```



```{r socmed, fig.cap= "Consensus by loosely estimated social media use (Facbook and Twitter/X) per year." }
checkRelationship(variable = "socmed_proportionate", var_label = "Proportionate Social Media Use")
```

Figure \@ref(fig:socmed) shows that there is a small relationship between social media use and consensus independence, such that people who used social media more often were more persuaded by a consensus. The right hand panel shows that there was also a relationship between social media use and the difference in belief in an independent consensus versus dependent consensus, such that people who use social media less tended to be more persuaded by independence relative to dependence. 


# Political Identity

## All groups

```{r politics, message = FALSE, fig.cap="Consensus by political identity"}
pol_order <- c("anarch", "soc", "strongLib", "modLib", "slightLib", "middle", "slightCons", "modCons", "strongCons")
by_politics <- merge_data_dem %>%
  group_by(consensus, demographics_politics) %>%
summarise(mean_update = mean(update),n_obs  = n()/60, sd = sd(update), se = sd / sqrt(n()))

plot_politics <- plot_grouped_bar(by_politics, demographics_politics, x_label = "Political Identity", label_angle = 45, usage_ordering = pol_order)
plot_politics_with_num_obs <- add_num_obs(merge_data_dem, by_politics, plot_politics, demographics_politics, "demographics_politics", vjust = -1, usage_ordering = pol_order)
plot_politics_with_num_obs

```

Figure \@ref(fig:politics)  shows consensus effects as a function of all of the political identities selected in our study. Let’s try to break it down into some more informative categories. For example, whether the identity was associated with the Left, Right, or Middle.
\pagebreak

## Left, centre, right
```{r political-dir, fig.cap="Consensus as a function of political direction"}
pol_group_order <- c("Left wing", "Centre", "Right wing")
d_political_group <- merge_data_dem %>%
  group_by(consensus, political_group) %>%
summarise(mean_update = mean(update),n_obs  = n()/60, sd = sd(update), se = sd / sqrt(n()))

plot_politics_group <- plot_grouped_bar(d_political_group, political_group, x_label = "Politics", label_angle = 45, usage_ordering = pol_group_order)
plot_politics_group_with_num_obs <- add_num_obs(merge_data_dem, d_political_group, plot_politics_group, political_group, "political_group", vjust = -1, usage_ordering = pol_group_order)

plot_politics_group_with_num_obs

```

Figure \@ref(fig:political-dir) shows that participants who identified with the political center appeared to be more convinced by consensus generally, and had a slightly smaller difference between dependent consensus and independent consensus, although the unbalanced sample sizes makes comparison difficult. These unbalanced sample sizes are essential to keep in mind while interpreting these graphs, as they are unlikely to be representative of the political groups with smaller sample sizes (e.g., Right wing groups).

\pagebreak

## Political Scale (Strong Left to Strong Right)
```{r political-scale, fig.cap="Consensus by strength of identification with Left versus Right"}
d_pol_scale <- merge_data_dem %>%
  filter(!demographics_politics %in% c("anarch", "soc"))

checkRelationship(data = d_pol_scale, variable = "political_scale", var_label = "Political Scale (Strong Left to Strong Right)")
```

Most of the political identities that were selected can be transformed to a linear scale of strong left (-3), moderate left (-2), slight left (-2), middle (0), slight conservative (1), moderate conservative (2), strong conservative (3). We removed the two participants who did not fit on this scale (as they selected “anarchist”and “socialist”).

The left side of Figure \@ref(fig:political-scale) shows that people tended to be slightly less convinced by a contested consensus as they identified more strongly with the Right. The right side of the figure shows that participants who more strongly associated with the Left tended to show a greater preference for independence compared to dependence. However, there was substantial uncertainty, with lots of overlap between the confidence intervals (shaded area) of the linear models (lines). 

\pagebreak

## Political Strength

Another way we can examine political aﬀiliation is in the absolute strength of political identification, where those who identified with the Center were coded as 0, and those who identified strongly with either the Left or the Right were coded as 3.

```{r political-strength, fig.cap="Consensus by political strength."}
checkRelationship(data = d_pol_scale, variable = "political_strength", var_label = "Political Strength (Centre to Strong)")
```
 
 Figure \@ref(fig:political-strength) shows no obvious relationship between political strength and a standard consensus effect (left), however participants with stronger political aﬀiliations tended to have a stronger preference for independence compared to dependence (right). Once again though, this difference was very noisy, with high overlap between the 95% confidence intervals.

\pagebreak

# Age

It is plausible that people might interact differently with a consensus presented in an online environment based on their age. The right side of Figure \@ref(fig: age) shows that there is unlikely to be an interaction between age and the standard consensus effect. The right side, however, suggests that older participants showed a larger preference for an independent consensus compared to a dependent consensus. However, as with all of the independent vs. dependent comparisons so far, the results appear highly uncertain.

```{r age, fig.cap= "Consensus by Age."}
checkRelationship(variable = "demographics_age", var_label = "age")
```


## Summary of qualitative analyses
In general, while we cannot make specific conclusions about whether any of these demographic variables underlie individual variation, these plots suggest that the independent vs. dependent comparison may vary based on demographic characteristics. This makes sense, given that our main analyses found that there was more individual variation for this comparison than the consensus vs no consensus one. In the next comparison, we perform a more rigorous quantitative test of the relationship between these demographic variables and consensus effects.

\pagebreak

# Model Comparison

Our primary quantitative analysis was a comparison of six additional Bayesian mixed effects models with the same structure as our main group level analyses, including random intercepts for each subject. One reason why these analyses are more rigorous than the lines of best fit drawn in the plots above is that they consider all of the trials in the experiment (instead of just the means).  These models all included the same outcome variable (beliefs after seeing the tweets), as well as the predictor of their prior belief in the claim, (“pre”), and an interaction with the consensus condition (consensus). As is in the main analyses, we split the data into our two comparisons of interest; contested vs. independent and dependent vs. independent, running all of the models separately for each comparison. We have detailed these analyses in the supplementary materials, but we will also describe the analyses and their results here. The demographic variables that we analyzed were: 

- Age: See see Figure \@ref(fig:age). Numeric age of participants. 
- Political group: See Figure \@ref(fig:political-group). Whether the participant selected a political identity aligned with the Left (Slight Left, Moderate Left, Strong Left, Socialist, Anarchist) , Center, or Right (slight right, moderate right, strong right). It is important to mention that our sample was not balanced, and there were substantially more people identifying with the Left compared to the Right and Middle. 
- Political scale: See Figure \@ref(fig:political-scale) Most of the political identities that were selected can be transformed to a linear scale of strong left (-3), moderate left (-2), slight left (-2), middle (0), slight conservative (1), moderate conservative (2),  strong conservative (3). We removed the two participants who did not fit on this scale (as they selected “anarchist” and “socialist”). 
- Political strength: See Figure \@ref(fig:political-scale). Same as political scale, but transformed so that it is the absolute value, therefore on a scale from 0 (no right/left political preference) to 3 (strong), regardless of direction or affiliation. 
- Education: See bottom panel of Figure \@ref(fig:education). Participants could select a number of options to indicate their level of education. In the current sample, "secondary"  "masters"  "bachelors"  "ged"   "vocational", and "doctorate" were selected. In its raw form, these options were not particularly informative, so we chose to divide participants into either university educated (bachelors, masters, or doctorate), or not (secondary, ged, or vocational). We thought that this would be an informative distinction since university educated students would be more likely to have had training interpreting scientific or other scholarly sources, so the primary sources in the consensus (universities and news companies) may have a different effect on their belief updating.  
- Social Media: See Figure \@ref(fig:socmed). An estimate of how much each participant used social media each year based on their response, where daily = 365, weekly = 52, monthly = 12, rarely = 1, and never = 0. We then aggregated the scores from both social media platforms to create a combined social media variable. This is, of course, not particularly precise, but may be able to pick up on some associations between social media use and sensitivity to consensus effects. 

We then performed a model comparison to determine whether the models including <demographic variable> X consensus interactions outperformed the previously best performing model reported in the main text. In other words, we were interested in whether including each respective demographic variable improved the model performance over and above the models that only included prior, claim type, and consensus.

\pagebreak


```{r}
formatModelComparison = function(mc_data){
  table <- mc_data %>% 
    mutate(model = case_when(
      model == "group-prior" ~ "pre",
      model == "group-prior-consensus" ~ "pre + consensus",
      model == "group-prior-consensus-claim" ~ "pre + consensus + claim type",
      model == "group-prior-consensusXclaim" ~ "pre + consensus X claim type",
      model == "group-prior-consensus*age" ~ "pre + consensus X age",
      model == "group-prior-consensus*unieducation" ~ "pre + conesnsus X university education",
      model == "group-prior-consensus*socmedprop" ~ "pre + consensus X social media",
      model == "group-prior-consensus*politcalscale" ~ "pre + consensus X political scale",
      model == "group-prior-consensus*politcalstrength" ~ "pre + consensus X political strength",
      model == "group-prior-consensus*politcalgroup" ~ "pre + consensus X political group"
    ))%>%
    rename(Model = model, LOOIC = all_looic, SE = all_se, Rank = rank)
  
    # format nice
    knitr::kable(table, format = "pandoc", escape = FALSE, row.names = TRUE)

}
```


## Independent vs. contested
```{r}
load(here("analyses/derived-data/group_output_combined.Rdata"))
mc_rm_dependent <- model_LOOICs %>%
  filter(excluded_condition == "dependent", !is_follow_up ) %>% 
  mutate(rank = rank(all_looic)) %>%
  select(-is_follow_up, -is_demographic, -excluded_condition)

formatModelComparison(mc_rm_dependent)

```

These model comparisons showed that the only models that performed better than the originally best performing models (consensus X claim type for independent vs. contested and consensus + claim type for independent vs. dependent) were those that included interactions with political scale and political strength. These results suggest that the extent to which people identified strongly with the left vs. right, and the absolute strength of political alignment may each contribute to the extent to which people were persuaded by an independent consensus trials relative to contested trials. To follow up the performance of these models, we inspected their coefficients to identify the strength and direction of any effects of the political variables. 

```{r}
formatOutput = function(output, demographic_variable) {
  sum_output <- summary(output, prob = .89)
  fixed_effects <- sum_output$fixed
  rownames(fixed_effects) <- c(
    "Intercept",
    "Pre",
      "Consensus$_{\\text{Independent}}$ ",
    demographic_variable,
    paste0(
      "Consensus$_{\\text{Independent}}$ X ",
      demographic_variable
    )
  )
  fixed_effects <-fixed_effects %>%
    rename(Error = Est.Error) %>%
    select(-Bulk_ESS,-Tail_ESS,-Rhat)
  
  knitr::kable(fixed_effects, format = "pandoc", escape = FALSE, row.names = TRUE)
 }
```


## Consensus X Political Scale (Strong Left to Strong Right)

```{r}
load(here("analyses/02_output/group-prior-consensus*politcalscale-rm-dependent.Rdata"))
formatOutput(output, "Political Scale")


```

The positive consensus X political scale interaction suggests that the difference between independent trials and contested trials increased the more strongly people identified with being conservative. In other words, people were more persuaded by a standard consensus if they were more conservative. Looking at the plot, this interaction seemed to be driven by conservatives having larger reductions in belief after contested trials, rather than being more persuaded by independent trials. However, it is important to note that although model comparison suggested that this was the best model, the interaction itself had a small effect size (estimate: .45) and was highly uncertain, with the 89% credible intervals overlapping substantially with zero. The basic trend underlying the interaction is shown in Figure 6, but note that the line of best fit and uncertainty metric are derived using different methods. 

## Political Strength (Centre to Strong)

```{r}
load(here("analyses/02_output/group-prior-consensus*politcalstrength-rm-dependent.Rdata"))
formatOutput(output, "Political Strength")
```

The negative Consensus X Political Strength interaction suggests that people with stronger political beliefs were less sensitive to consensus. However, the effect size was tiny (as illustrated by the near parallel lines in the plot and an estimate of -0.03) and highly uncertain. 

## Indpendent vs. dependent

```{r}
mc_rm_contested <- model_LOOICs %>%
  filter(excluded_condition == "contested", !is_follow_up) %>% 
  mutate(rank = rank(all_looic)) %>%
    select(-is_follow_up, -is_demographic, -excluded_condition)


formatModelComparison(mc_rm_contested)
```


### Political Scale (Strong Left to Strong Right)

```{r}
load(here("analyses/02_output/group-prior-consensus*politcalscale-rm-contested.Rdata"))
formatOutput(output, "Political Scale")
```

The negative Consensus X Political Scale interaction suggests that the more left leaning the participants were, the less persuaded they were by a dependent consensus on average. This effect was around the same size, with similarly high uncertainty, as the contested vs. independence comparison. Interestingly however, the main effect of independence reduced and became much more uncertain compared to the consensus + claim type model, suggesting that the interaction explained much of the variance that was previously associated with the main effect of consensus. 


### Political Strength (Centre to Strong)

```{r}
load(here("analyses/02_output/group-prior-consensus*politcalstrength-rm-contested.Rdata"))
formatOutput(output, "Political Strength")
```

The Consensus X Political Strength interaction suggested that people who had stronger political identity (regardless of whether they identified more with the Left or the Right), showed a stronger preference for an independent consensus compared to a dependent consensus. Once again however, this effect was small and unreliable. 

## Summary of Model Comparison

Our model comparisons suggest that two political variables were the most likely to be underlying individual variation in consensus effects: absolute strength of political identification and direction of political identification. However, the effect sizes on the coefficients were very small and highly uncertain (which makes sense because our study was not set up to detect these differences), indicating a high probability that the true value could be zero and therefore, no credible statistical relationship. Taken together, despite the fact these political variables performed well in the model comparison, statistical evidence was still weak and highly uncertain, meaning we are unable to conclude that any of these variables were driving individual differences– especially when considering issues with our design which we outline in the next paragraph.

There are a number of additional reasons why these results should be interpreted with substantial caution. First, none of them were pre-registered. Second, we did not collect a politically balanced sample, meaning that there were far more people who identified to the Left of the political spectrum compared to the Right. Indeed, there was only one participant who identified strongly with the Right, compared to 24 who identified strongly with the Left. Although the high number of trials per participant means that the estimates for that one far right participant should be reasonably reliable, we have clearly not sampled a representative range of conservative participants, so inferences should not be made about the whole political group. Indeed, looking at the plots it is clear that there is a high degree of variability within the Left leaning groups with a larger sample size, suggesting that had we recruited a smaller sample, there is a reasonable possibility that we may have observed similar results as the conservative sample simply by chance. Third, many of these analyses relied on transforming the original response, such as categorizing the education variable, which can reduce the sensitivity of these analyses as well as reflect a high researcher degree of freedom. Therefore, null results could simply be because of how the data was collected and transformed, instead of because no relationship actually exists. These results could be used to motivate future research by outlining possible explanations for these effects, but they should be interpreted with a high degree of caution and not be used to draw any generalizing  conclusions about any demographic subgroups. For these reasons, we have not included these analyses in the main text. We include them in the Supplementary Materials in the hope that these results provide inspiration for future research that can test these associations more rigorously. 

